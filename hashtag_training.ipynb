{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hashtag_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-2-3-cpu.2-3.m56",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-cpu.2-3:m56"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/cs486_project/blob/main/hashtag_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip64gceaZmbn"
      },
      "source": [
        "Standard imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCK66SufVGP",
        "outputId": "d86b4df5-34d5-4952-bfb8-92f38b883f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lThdnLZ7DDaa"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Embedding, Dot, Add\n",
        "from keras.activations import sigmoid\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.applications.vgg19 import VGG19\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pickle\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErMs_r5jZpRj"
      },
      "source": [
        "Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELdcuZdMBJYk"
      },
      "source": [
        "# project_path = './'\n",
        "project_path = '/content/drive/My Drive/AI(CS486)'\n",
        "IMG_DIM = 512\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "feature_size = 1000\n",
        "\n",
        "hashtags = []\n",
        "with open(os.path.join(project_path, 'data', 'hashtags.txt')) as f:\n",
        "  for line in f:\n",
        "    hashtags.append(line)\n",
        "\n",
        "num_imgs = 2936 # n: number of images \n",
        "num_hashtags = len(hashtags) # m\n",
        "\n",
        "inputs = np.load(os.path.join(project_path, 'data', 'train_inputs.npy')).tolist()\n",
        "\n",
        "inputs_map = {}\n",
        "for input in inputs:\n",
        "  if inputs_map.get(input[0]) == None:\n",
        "    inputs_map[input[0]] = { input[1] : True }\n",
        "  else:\n",
        "    inputs_map[input[0]][input[1]] = True\n",
        "\n",
        "# labels\n",
        "labels = [1 for i in range(len(inputs))]\n",
        "\n",
        "for i in range(num_imgs):\n",
        "  for j in range(num_hashtags):\n",
        "    if inputs_map.get(i) == None:\n",
        "      labels.append(0)\n",
        "      inputs.append([i, j])\n",
        "      inputs_map[i] = { j : True }\n",
        "    elif inputs_map[i].get(j) == None: # false label\n",
        "      labels.append(0)\n",
        "      inputs.append([i, j])\n",
        "      inputs_map[i][j] = True\n",
        "      \n",
        "labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "inputs = np.array(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIhTYp6O_wUy"
      },
      "source": [
        "batch_sizes = [256]\n",
        "# batch_sizes = [32, 64]\n",
        "# feature_sizes = [1000, 10000]\n",
        "feature_sizes = [1000]\n",
        "num_epochs = 100\n",
        "init_lrs = [1e-2, 1e-3, 1e-4]\n",
        "# init_lrs = [1e-2, 1e-3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clL_yLGlEjE_"
      },
      "source": [
        "class HashtagPredictor(Model):\n",
        "  def __init__(self, num_imgs, num_hashtags, feature_size, **kwargs):\n",
        "    super(HashtagPredictor, self).__init__(**kwargs)\n",
        "    self.num_imgs = num_imgs + 1 # + 1 for test image ..?\n",
        "    self.num_hashtags = num_hashtags\n",
        "    self.feature_size = feature_size\n",
        "    self.img_embedding = Embedding(self.num_imgs, self.feature_size, embeddings_initializer=\"he_normal\", )\n",
        "    self.img_bias = Embedding(self.num_imgs, 1)\n",
        "    self.hashtag_embedding = Embedding(self.num_hashtags, self.feature_size, embeddings_initializer=\"he_normal\")\n",
        "    self.hashtag_bias = Embedding(self.num_hashtags, 1)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    img_matrix = self.img_embedding(inputs[:, 0])\n",
        "    img_bias = self.img_bias(inputs[:, 0])\n",
        "    hashtag_matrix = self.hashtag_embedding(inputs[:, 1])\n",
        "    hashtag_bias = self.hashtag_bias(inputs[:, 1])\n",
        "    raw_preds = Dot(axes=1)([img_matrix, hashtag_matrix])\n",
        "    preds_with_bias = Add()([raw_preds, img_bias, hashtag_bias])\n",
        "    preds = sigmoid(raw_preds)\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn1O2_0TNYA7"
      },
      "source": [
        "for b, batch_size in enumerate(batch_sizes):\n",
        "  for lr_ind, init_lr in enumerate(init_lrs):\n",
        "    for feat_ind, feature_size in enumerate(feature_sizes):\n",
        "        print(f'Creating model with batch size: {batch_size}, learning rate: {init_lr}, feature size: {feature_size}')\n",
        "        model = HashtagPredictor(num_imgs, num_hashtags, feature_size)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=init_lr), metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(\n",
        "            x=inputs,\n",
        "            y=labels,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "            verbose=1,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "        model.save(os.path.join(project_path, 'models', f'model_bs_{batch_size}_lr_{init_lr}_fs_{feature_size}'))\n",
        "\n",
        "        with open(os.path.join(project_path, 'histories', f'history_bs_{batch_size}_lr_{init_lr}_fs_{feature_size}'), 'wb') as f:\n",
        "          pickle.dump(history.history, f)\n",
        "        \n",
        "        clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc1VIUg9aWMS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}